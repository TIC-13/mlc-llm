{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "45aff530-c12d-4a22-808a-09af8a9a836e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pinecone-client\n",
      "  Using cached pinecone_client-4.0.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting certifi>=2019.11.17 (from pinecone-client)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting tqdm>=4.64.1 (from pinecone-client)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m673.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m672.0 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions>=3.7.4 (from pinecone-client)\n",
      "  Using cached typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting urllib3>=1.26.0 (from pinecone-client)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Using cached pinecone_client-4.0.0-py3-none-any.whl (214 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m209.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hUsing cached typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, tqdm, certifi, pinecone-client\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.1\n",
      "    Uninstalling urllib3-2.2.1:\n",
      "      Successfully uninstalled urllib3-2.2.1\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.2\n",
      "    Uninstalling tqdm-4.66.2:\n",
      "      Successfully uninstalled tqdm-4.66.2\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2023.11.17\n",
      "    Uninstalling certifi-2023.11.17:\n",
      "      Successfully uninstalled certifi-2023.11.17\n",
      "  Attempting uninstall: pinecone-client\n",
      "    Found existing installation: pinecone-client 4.0.0\n",
      "    Uninstalling pinecone-client-4.0.0:\n",
      "      Successfully uninstalled pinecone-client-4.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "litellm 1.16.21 requires certifi<2024.0.0,>=2023.7.22, but you have certifi 2024.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed certifi-2024.2.2 pinecone-client-4.0.0 tqdm-4.66.4 typing-extensions-4.11.0 urllib3-2.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (0.9.27)\n",
      "Requirement already satisfied: transformers in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (4.40.2)\n",
      "Requirement already satisfied: accelerate in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (0.30.0)\n",
      "Requirement already satisfied: bitsandbytes in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (0.43.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from llama-index) (3.9.5)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from llama-index) (4.12.3)\n",
      "Requirement already satisfied: dataclasses-json in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from llama-index) (0.6.5)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from llama-index) (1.2.14)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from llama-index) (2024.3.1)\n",
      "Requirement already satisfied: httpx in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from llama-index) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from llama-index) (3.3)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from llama-index) (3.8.1)\n",
      "Requirement already satisfied: numpy in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from llama-index) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from llama-index) (1.23.6)\n",
      "Requirement already satisfied: pandas in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from llama-index) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from llama-index) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from llama-index) (8.3.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from llama-index) (0.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from llama-index) (4.11.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from llama-index) (0.9.0)\n",
      "Requirement already satisfied: filelock in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from transformers) (0.23.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from transformers) (2024.4.16)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: psutil in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from accelerate) (2.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.9.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.2->llama-index) (2.5)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from deprecated>=1.2.9.3->llama-index) (1.16.0)\n",
      "Requirement already satisfied: click in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from openai>=1.1.0->llama-index) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from openai>=1.1.0->llama-index) (1.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from openai>=1.1.0->llama-index) (2.7.1)\n",
      "Requirement already satisfied: sniffio in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from openai>=1.1.0->llama-index) (1.3.0)\n",
      "Requirement already satisfied: certifi in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from httpx->llama-index) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from httpx->llama-index) (1.0.5)\n",
      "Requirement already satisfied: idna in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from httpx->llama-index) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index) (2.2.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index) (3.0.3)\n",
      "Requirement already satisfied: sympy in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: jinja2 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from dataclasses-json->llama-index) (3.21.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from pandas->llama-index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from pandas->llama-index) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from pandas->llama-index) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index) (2.18.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pinecone-client\n",
    "!pip install llama-index transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6ce3e8ea-3630-40bd-8a7a-c9b66f154f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autollm in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (0.1.10)\n",
      "Requirement already satisfied: langchain in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (0.1.19)\n",
      "Requirement already satisfied: llama-index==0.9.27 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from autollm) (0.9.27)\n",
      "Requirement already satisfied: litellm==1.16.21 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from autollm) (1.16.21)\n",
      "Requirement already satisfied: uvicorn in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from autollm) (0.29.0)\n",
      "Requirement already satisfied: fastapi in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from autollm) (0.110.2)\n",
      "Requirement already satisfied: python-dotenv in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from autollm) (1.0.1)\n",
      "Requirement already satisfied: httpx in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from autollm) (0.27.0)\n",
      "Requirement already satisfied: lancedb==0.3.4 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from autollm) (0.3.4)\n",
      "Requirement already satisfied: deprecation in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from lancedb==0.3.4->autollm) (2.1.0)\n",
      "Requirement already satisfied: pylance==0.8.17 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from lancedb==0.3.4->autollm) (0.8.17)\n",
      "Requirement already satisfied: ratelimiter~=1.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from lancedb==0.3.4->autollm) (1.2.0.post0)\n",
      "Requirement already satisfied: retry>=0.9.2 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from lancedb==0.3.4->autollm) (0.9.2)\n",
      "Requirement already satisfied: tqdm>=4.1.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from lancedb==0.3.4->autollm) (4.66.2)\n",
      "Requirement already satisfied: aiohttp in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from lancedb==0.3.4->autollm) (3.9.5)\n",
      "Requirement already satisfied: pydantic>=1.10 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from lancedb==0.3.4->autollm) (2.7.1)\n",
      "Requirement already satisfied: attrs>=21.3.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from lancedb==0.3.4->autollm) (23.2.0)\n",
      "Requirement already satisfied: semver>=3.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from lancedb==0.3.4->autollm) (3.0.2)\n",
      "Requirement already satisfied: cachetools in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from lancedb==0.3.4->autollm) (5.3.3)\n",
      "Requirement already satisfied: pyyaml>=6.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from lancedb==0.3.4->autollm) (6.0.1)\n",
      "Requirement already satisfied: click>=8.1.7 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from lancedb==0.3.4->autollm) (8.1.7)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from lancedb==0.3.4->autollm) (2.31.0)\n",
      "Requirement already satisfied: overrides>=0.7 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from lancedb==0.3.4->autollm) (7.4.0)\n",
      "Requirement already satisfied: certifi<2024.0.0,>=2023.7.22 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from litellm==1.16.21->autollm) (2023.11.17)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from litellm==1.16.21->autollm) (7.1.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from litellm==1.16.21->autollm) (3.1.3)\n",
      "Requirement already satisfied: openai>=1.0.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from litellm==1.16.21->autollm) (1.23.6)\n",
      "Requirement already satisfied: tiktoken>=0.4.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from litellm==1.16.21->autollm) (0.6.0)\n",
      "Requirement already satisfied: tokenizers in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from litellm==1.16.21->autollm) (0.19.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index==0.9.27->autollm) (2.0.30)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from llama-index==0.9.27->autollm) (4.12.3)\n",
      "Requirement already satisfied: dataclasses-json in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from llama-index==0.9.27->autollm) (0.6.5)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from llama-index==0.9.27->autollm) (1.2.14)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from llama-index==0.9.27->autollm) (2024.3.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from llama-index==0.9.27->autollm) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from llama-index==0.9.27->autollm) (3.3)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from llama-index==0.9.27->autollm) (3.8.1)\n",
      "Requirement already satisfied: numpy in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from llama-index==0.9.27->autollm) (1.26.4)\n",
      "Requirement already satisfied: pandas in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from llama-index==0.9.27->autollm) (2.2.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from llama-index==0.9.27->autollm) (8.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from llama-index==0.9.27->autollm) (4.9.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from llama-index==0.9.27->autollm) (0.9.0)\n",
      "Requirement already satisfied: pyarrow>=10 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from pylance==0.8.17->lancedb==0.3.4->autollm) (16.0.0)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.38 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from langchain) (0.0.38)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.52 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from langchain) (0.1.52)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from langchain) (0.1.56)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from aiohttp->lancedb==0.3.4->autollm) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from aiohttp->lancedb==0.3.4->autollm) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from aiohttp->lancedb==0.3.4->autollm) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from aiohttp->lancedb==0.3.4->autollm) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from dataclasses-json->llama-index==0.9.27->autollm) (3.21.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from pydantic>=1.10->lancedb==0.3.4->autollm) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from pydantic>=1.10->lancedb==0.3.4->autollm) (2.18.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from requests>=2.31.0->lancedb==0.3.4->autollm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from requests>=2.31.0->lancedb==0.3.4->autollm) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from requests>=2.31.0->lancedb==0.3.4->autollm) (2.2.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index==0.9.27->autollm) (3.0.3)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from fastapi->autollm) (0.37.2)\n",
      "Requirement already satisfied: anyio in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from httpx->autollm) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from httpx->autollm) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from httpx->autollm) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from httpcore==1.*->httpx->autollm) (0.14.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.2->llama-index==0.9.27->autollm) (2.5)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from deprecated>=1.2.9.3->llama-index==0.9.27->autollm) (1.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm==1.16.21->autollm) (3.18.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from jinja2<4.0.0,>=3.1.2->litellm==1.16.21->autollm) (2.1.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain) (2.4)\n",
      "Requirement already satisfied: joblib in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.9.27->autollm) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.9.27->autollm) (2024.4.16)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from openai>=1.0.0->litellm==1.16.21->autollm) (1.9.0)\n",
      "Requirement already satisfied: decorator>=3.4.2 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from retry>=0.9.2->lancedb==0.3.4->autollm) (5.1.1)\n",
      "Requirement already satisfied: py<2.0.0,>=1.4.26 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from retry>=0.9.2->lancedb==0.3.4->autollm) (1.11.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index==0.9.27->autollm) (1.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from pandas->llama-index==0.9.27->autollm) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from pandas->llama-index==0.9.27->autollm) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from pandas->llama-index==0.9.27->autollm) (2024.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from tokenizers->litellm==1.16.21->autollm) (0.23.0)\n",
      "Requirement already satisfied: filelock in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm==1.16.21->autollm) (3.13.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index==0.9.27->autollm) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install autollm langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5ae53e3e-259b-45a0-bdad-9a3517ad833b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaleido in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (0.2.1)\n",
      "Requirement already satisfied: cohere in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (5.3.5)\n",
      "Requirement already satisfied: langchain in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (0.1.19)\n",
      "Requirement already satisfied: pdfminer.six in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (20231228)\n",
      "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from cohere) (1.9.4)\n",
      "Requirement already satisfied: httpx>=0.21.2 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from cohere) (0.27.0)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from cohere) (0.4.0)\n",
      "Requirement already satisfied: pydantic>=1.9.2 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from cohere) (2.7.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from cohere) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from cohere) (0.19.1)\n",
      "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from cohere) (2.31.0.20240406)\n",
      "Requirement already satisfied: typing_extensions>=4.0.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from cohere) (4.9.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from langchain) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from langchain) (0.6.5)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.38 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from langchain) (0.0.38)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.52 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from langchain) (0.1.52)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from langchain) (0.1.56)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from langchain) (8.3.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from pdfminer.six) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from pdfminer.six) (42.0.7)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: anyio in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from httpx>=0.21.2->cohere) (4.2.0)\n",
      "Requirement already satisfied: certifi in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from httpx>=0.21.2->cohere) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from httpx>=0.21.2->cohere) (1.0.5)\n",
      "Requirement already satisfied: idna in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from httpx>=0.21.2->cohere) (3.7)\n",
      "Requirement already satisfied: sniffio in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from httpx>=0.21.2->cohere) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from pydantic>=1.9.2->cohere) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from pydantic>=1.9.2->cohere) (2.18.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->cohere) (2.2.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from tokenizers<0.20,>=0.19->cohere) (0.23.0)\n",
      "Requirement already satisfied: pycparser in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
      "Requirement already satisfied: filelock in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20,>=0.19->cohere) (3.13.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20,>=0.19->cohere) (2024.3.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20,>=0.19->cohere) (4.66.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain) (2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install kaleido cohere langchain pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80a4bc91-5d4c-4373-bd02-c662df652aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_TOKEN=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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88b17a2e-d538-47e3-a399-c5e45fb913b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autollm.utils.document_reading import read_files_as_documents\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pinecone\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "from mlc_llm import ChatModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "54e4603b-6f0c-4899-a7ed-2fb8c322fdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-08 22:06:12--  https://manuals.coolblue.nl/e0/motorola-edge-30-ultra-256gb-grijs-5g.pdf\n",
      "Resolving manuals.coolblue.nl (manuals.coolblue.nl)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108.156.83.77, 108.156.83.98, 108.156.83.88, ...\n",
      "Connecting to manuals.coolblue.nl (manuals.coolblue.nl)|108.156.83.77|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2160720 (2,1M) [application/octet-stream]\n",
      "Saving to: ‘motorola-edge-30-ultra-256gb-grijs-5g.pdf’\n",
      "\n",
      "motorola-edge-30-ul 100%[===================>]   2,06M  2,22MB/s    in 0,9s    \n",
      "\n",
      "2024-05-08 22:06:14 (2,22 MB/s) - ‘motorola-edge-30-ultra-256gb-grijs-5g.pdf’ saved [2160720/2160720]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://manuals.coolblue.nl/e0/motorola-edge-30-ultra-256gb-grijs-5g.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f1aa8f77-068f-46a3-b3cb-eb8bd00ea3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data && mv *.pdf data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81f44aa3-806c-456a-95ec-86bcbe0df436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-09 00:46:37,669 - autollm - INFO - Reading files from data/..\n",
      "Loading files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.92s/file]\n",
      "2024-05-09 00:46:44,624 - autollm - INFO - Found 1 'document(s)'.\n"
     ]
    }
   ],
   "source": [
    "required_exts = [\".pdf\"]\n",
    "documents = read_files_as_documents(input_dir=\"data/\", required_exts=required_exts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be18b636-b2e9-49ae-b538-70d8c649c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=40,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f12d457-89c4-4718-be35-3630039eb28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.split_text(re.sub(\"\\\\n[0-9]+\", \"\", documents[0].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fb52ca1-e28d-413c-a0ca-f90686f2e0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'User Guide\\n\\n\\x0c© 2022 Motorola Mobility LLC. All rights reserved.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f68034f-fa2e-4282-bf17-2883bdf4c22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgpt/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 384)\n",
       "    (token_type_embeddings): Embedding(2, 384)\n",
       "    (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-small-en-v1.5')\n",
    "model = AutoModel.from_pretrained('BAAI/bge-small-en-v1.5')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd8e8673-8c63-43bc-9193-efdeb2fb2bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings: tensor([[-0.0322, -0.0685, -0.0062,  ..., -0.0562,  0.0705, -0.0242],\n",
      "        [-0.0956, -0.0878,  0.0068,  ...,  0.0423,  0.1237, -0.0125],\n",
      "        [-0.0370, -0.0277,  0.0332,  ...,  0.0303,  0.0544, -0.0170],\n",
      "        [ 0.0062, -0.0548,  0.0408,  ..., -0.0252,  0.0500, -0.0154]])\n"
     ]
    }
   ],
   "source": [
    "# Sentences we want sentence embeddings for\n",
    "sentences = [\"This is bad\", \"This is good\", \"I love it\", \"I hate_it\"]\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "    # Perform pooling. In this case, cls pooling.\n",
    "    sentence_embeddings = model_output[0][:, 0]\n",
    "# normalize embeddings\n",
    "sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "print(\"Sentence embeddings:\", sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "575735ed-20c9-4a7a-a67d-f8b14fbe7eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([384])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97edd05a-93cf-42d1-a897-927065625f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"9fd24bed-2037-40d9-b409-4d3c088f7f35\"\n",
    "pc = pinecone.Pinecone(\n",
    "    api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "7b8bbb40-997c-49e0-879f-e52f4fe916c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.create_index(\n",
    "    name='rag-test-1', \n",
    "    dimension=384, \n",
    "    metric='cosine',\n",
    "    spec=pinecone.ServerlessSpec(\n",
    "        cloud='aws', \n",
    "        region='us-east-1'\n",
    "    ) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03d6315a-a50e-41a3-9de8-7d429d095afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pc.Index('rag-test-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae0465ea-36a2-47a3-9e45-efab4ef922ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 993}},\n",
       " 'total_vector_count': 993}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ce21b38-4c95-4515-805b-643d9fa7fade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_texts(texts):\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "        sentence_embeddings = model_output[0][:, 0]\n",
    "    return torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "732a71e3-6e9f-448f-bc8d-33b067becb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = embed_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4e13a15-6665-4f30-b4f8-7f51652ecc85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectors) == len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb188a40-3d9d-44ed-a3ec-46d6db52c833",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How to charge?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cff8b159-2286-4275-a54e-5ca42f2edeec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.015092134475708008,\n",
       " -0.05926612392067909,\n",
       " -0.013547316193580627,\n",
       " -0.01438846904784441,\n",
       " 0.0061742221005260944,\n",
       " -0.04573896899819374,\n",
       " 0.037111494690179825,\n",
       " 0.03133055567741394,\n",
       " 0.02362605556845665,\n",
       " 0.019886642694473267,\n",
       " 0.019794141873717308,\n",
       " 0.02755310945212841,\n",
       " -0.043975356966257095,\n",
       " 0.04781898856163025,\n",
       " 0.05422568321228027,\n",
       " 0.06277812272310257,\n",
       " -0.013539250940084457,\n",
       " -0.06175018474459648,\n",
       " -0.07623101770877838,\n",
       " 0.044889286160469055,\n",
       " 0.028803467750549316,\n",
       " -0.056456319987773895,\n",
       " -0.0307297520339489,\n",
       " -0.03815346211194992,\n",
       " -0.02310054562985897,\n",
       " 0.008246973156929016,\n",
       " 0.006733294110745192,\n",
       " 0.013329464942216873,\n",
       " -0.05286294221878052,\n",
       " -0.09642944484949112,\n",
       " 0.07833661139011383,\n",
       " -0.014718685299158096,\n",
       " 0.011664575897157192,\n",
       " 0.036141905933618546,\n",
       " 0.014925087802112103,\n",
       " -0.03104899823665619,\n",
       " -0.035499557852745056,\n",
       " -0.05144558101892471,\n",
       " -0.011069653555750847,\n",
       " 0.00242385221645236,\n",
       " 0.03968678042292595,\n",
       " 0.03669983893632889,\n",
       " -0.09162604808807373,\n",
       " -0.07529009133577347,\n",
       " -0.006815750617533922,\n",
       " -0.030405471101403236,\n",
       " 0.05035196989774704,\n",
       " 0.01720179058611393,\n",
       " 0.10367920249700546,\n",
       " 0.04965590685606003,\n",
       " 0.03862975910305977,\n",
       " 0.0005775283207185566,\n",
       " -0.004965833388268948,\n",
       " -0.06976059824228287,\n",
       " -0.035618118941783905,\n",
       " 0.05791157856583595,\n",
       " 0.0430670790374279,\n",
       " 0.01847720891237259,\n",
       " 0.04972042888402939,\n",
       " -0.05006077140569687,\n",
       " 0.005041234195232391,\n",
       " 0.009894355200231075,\n",
       " -0.11431461572647095,\n",
       " 0.11349831521511078,\n",
       " -0.03684093803167343,\n",
       " 0.01159245241433382,\n",
       " -0.0038012894801795483,\n",
       " 0.07125149667263031,\n",
       " -0.019517377018928528,\n",
       " 0.0862601175904274,\n",
       " -0.011967240832746029,\n",
       " -0.026381343603134155,\n",
       " -0.03957943618297577,\n",
       " 0.01301306951791048,\n",
       " 0.03570830076932907,\n",
       " -0.061266832053661346,\n",
       " 0.007076465059071779,\n",
       " 0.05981961637735367,\n",
       " 0.061566151678562164,\n",
       " 0.013798723928630352,\n",
       " -0.06435183435678482,\n",
       " -0.04040176421403885,\n",
       " -0.0034628049470484257,\n",
       " 0.08907627314329147,\n",
       " 0.012318434193730354,\n",
       " -0.013678938150405884,\n",
       " 0.022641917690634727,\n",
       " 0.012373263947665691,\n",
       " 0.008636536076664925,\n",
       " 0.005493460688740015,\n",
       " -0.004798968322575092,\n",
       " -0.008519190363585949,\n",
       " -0.06080995127558708,\n",
       " 0.0020857402123510838,\n",
       " -0.07300484925508499,\n",
       " 0.03347577154636383,\n",
       " 0.015028614550828934,\n",
       " 0.0017219830770045519,\n",
       " -0.1251973658800125,\n",
       " 0.3447958528995514,\n",
       " 0.021610476076602936,\n",
       " 0.06439310312271118,\n",
       " 0.033143483102321625,\n",
       " -0.04322018846869469,\n",
       " -0.03956357762217522,\n",
       " -0.06971890479326248,\n",
       " -0.04217035695910454,\n",
       " 0.04033273831009865,\n",
       " -0.05182529240846634,\n",
       " -0.019473593682050705,\n",
       " -0.049960702657699585,\n",
       " 0.008044376969337463,\n",
       " 0.0129588907584548,\n",
       " -0.07462999224662781,\n",
       " 0.042570121586322784,\n",
       " 0.1075560599565506,\n",
       " -0.06237112358212471,\n",
       " -0.02328580990433693,\n",
       " 0.004113755654543638,\n",
       " -0.08658744394779205,\n",
       " 0.0034770050551742315,\n",
       " 0.004604268353432417,\n",
       " -0.020518163219094276,\n",
       " -0.0007440689951181412,\n",
       " 0.04462449252605438,\n",
       " -0.04695093631744385,\n",
       " 0.04081964120268822,\n",
       " 0.07555924355983734,\n",
       " 0.0006600403576157987,\n",
       " 0.07980558276176453,\n",
       " 0.03796080872416496,\n",
       " -0.07016000151634216,\n",
       " -0.07730146497488022,\n",
       " 0.003926950506865978,\n",
       " 0.019988680258393288,\n",
       " -0.03182224556803703,\n",
       " 0.021762993186712265,\n",
       " 0.020219910889863968,\n",
       " 0.011687394231557846,\n",
       " 0.04918932542204857,\n",
       " -0.030453693121671677,\n",
       " -0.06779532134532928,\n",
       " 0.002135845134034753,\n",
       " -0.059902600944042206,\n",
       " -0.016546696424484253,\n",
       " 0.034106843173503876,\n",
       " 0.008150268346071243,\n",
       " 0.007643175311386585,\n",
       " 0.00794966984540224,\n",
       " -0.05637085437774658,\n",
       " -0.0011839608196169138,\n",
       " -0.014759036712348461,\n",
       " -0.01575302891433239,\n",
       " -0.06084732338786125,\n",
       " 0.04706122726202011,\n",
       " 0.05732284486293793,\n",
       " 0.061101216822862625,\n",
       " -0.039697203785181046,\n",
       " -0.017361797392368317,\n",
       " 0.011425896547734737,\n",
       " -0.003497083904221654,\n",
       " 0.009197586216032505,\n",
       " -0.054162319749593735,\n",
       " 0.10672323405742645,\n",
       " 0.010421432554721832,\n",
       " -0.06595500558614731,\n",
       " -0.0008390615694224834,\n",
       " -0.04249085858464241,\n",
       " -0.004623027518391609,\n",
       " -0.025094104930758476,\n",
       " -0.04359808936715126,\n",
       " 0.06681879609823227,\n",
       " -0.0761117935180664,\n",
       " 0.04304255545139313,\n",
       " 0.1267109364271164,\n",
       " -0.0504554882645607,\n",
       " 0.002911284100264311,\n",
       " 0.02470637485384941,\n",
       " 0.0016021315241232514,\n",
       " -0.022870104759931564,\n",
       " 0.016761986538767815,\n",
       " 0.021951818838715553,\n",
       " -0.015301543287932873,\n",
       " -0.021749073639512062,\n",
       " -0.002405809238553047,\n",
       " -0.08394943177700043,\n",
       " -0.02766694314777851,\n",
       " -0.05852186679840088,\n",
       " 0.002714399015530944,\n",
       " 0.018977848812937737,\n",
       " -0.025793327018618584,\n",
       " 0.01609363779425621,\n",
       " -0.049486368894577026,\n",
       " 0.030463777482509613,\n",
       " -0.04941653087735176,\n",
       " 0.028306635096669197,\n",
       " -0.02133866585791111,\n",
       " -0.012619026005268097,\n",
       " 0.012244307436048985,\n",
       " 0.0266746636480093,\n",
       " 0.08856888115406036,\n",
       " -0.053719017654657364,\n",
       " -0.044107209891080856,\n",
       " 0.047532591968774796,\n",
       " 0.04345810413360596,\n",
       " 0.047882940620183945,\n",
       " 0.04725795239210129,\n",
       " 0.04368758201599121,\n",
       " 0.054988693445920944,\n",
       " -0.022999482229351997,\n",
       " 0.06057240813970566,\n",
       " 0.0357135608792305,\n",
       " 0.1275389939546585,\n",
       " 0.028164243325591087,\n",
       " -0.04747837409377098,\n",
       " -0.017187636345624924,\n",
       " -0.01421502698212862,\n",
       " 0.029878083616495132,\n",
       " -0.0046982127241790295,\n",
       " -0.005085834302008152,\n",
       " -0.02345837466418743,\n",
       " -0.06609131395816803,\n",
       " 0.06713762879371643,\n",
       " -0.2697671055793762,\n",
       " 0.0690336599946022,\n",
       " 0.055189069360494614,\n",
       " 0.006241785828024149,\n",
       " -0.024977602064609528,\n",
       " -0.04384205490350723,\n",
       " 0.1121152937412262,\n",
       " -0.024483172222971916,\n",
       " -0.03154172748327255,\n",
       " 0.0642722025513649,\n",
       " 0.067870132625103,\n",
       " -0.10862479358911514,\n",
       " 0.039840780198574066,\n",
       " -0.03438805043697357,\n",
       " 0.04776566103100777,\n",
       " 0.06809845566749573,\n",
       " 0.029874162748456,\n",
       " 0.0369715616106987,\n",
       " 0.05131794884800911,\n",
       " 0.020834660157561302,\n",
       " -0.03300068527460098,\n",
       " -0.008913585916161537,\n",
       " 0.010335735976696014,\n",
       " -0.009732270613312721,\n",
       " 0.05019381269812584,\n",
       " -0.016908278688788414,\n",
       " 0.17538630962371826,\n",
       " 0.028375590220093727,\n",
       " 0.02549198642373085,\n",
       " -0.01187197957187891,\n",
       " 0.08616071194410324,\n",
       " -0.026350397616624832,\n",
       " -0.04112221673130989,\n",
       " -0.09613236039876938,\n",
       " -0.030994165688753128,\n",
       " 0.011272186413407326,\n",
       " -0.04959283024072647,\n",
       " 0.007309121545404196,\n",
       " 0.010345970280468464,\n",
       " -0.0009276348282583058,\n",
       " -0.011494460515677929,\n",
       " -0.001710040494799614,\n",
       " -0.05235046148300171,\n",
       " 0.005240337923169136,\n",
       " 0.009123200550675392,\n",
       " 0.06292521953582764,\n",
       " -0.06552086770534515,\n",
       " 0.009762692265212536,\n",
       " -0.004882464185357094,\n",
       " -0.04005386680364609,\n",
       " 0.034816939383745193,\n",
       " -0.0625707134604454,\n",
       " -0.012287365272641182,\n",
       " -0.015558172017335892,\n",
       " -0.00272513460367918,\n",
       " -0.051791973412036896,\n",
       " -0.04537096247076988,\n",
       " 0.025958659127354622,\n",
       " -0.03694295510649681,\n",
       " -0.013973737135529518,\n",
       " 0.038704175502061844,\n",
       " 0.025689352303743362,\n",
       " 0.003986716270446777,\n",
       " -0.02082807756960392,\n",
       " 0.04535677656531334,\n",
       " 0.012601529248058796,\n",
       " 0.022453520447015762,\n",
       " -0.004350797273218632,\n",
       " 0.006419549696147442,\n",
       " -0.029304765164852142,\n",
       " -0.027846399694681168,\n",
       " 0.019053272902965546,\n",
       " -0.019729362800717354,\n",
       " 0.03095846250653267,\n",
       " 0.008279132656753063,\n",
       " 0.03716583177447319,\n",
       " 0.042118899524211884,\n",
       " -0.006089366041123867,\n",
       " -0.0561138279736042,\n",
       " -0.02084348537027836,\n",
       " -0.023252101615071297,\n",
       " 0.022534552961587906,\n",
       " 0.019824182614684105,\n",
       " 0.0019466972444206476,\n",
       " -0.004590023308992386,\n",
       " 0.07507321983575821,\n",
       " -0.022394264116883278,\n",
       " -0.030284881591796875,\n",
       " 0.0007511351723223925,\n",
       " -0.08691935986280441,\n",
       " -0.05587220564484596,\n",
       " -0.004988866858184338,\n",
       " -0.00961386226117611,\n",
       " -0.0020702697802335024,\n",
       " -0.000374587660189718,\n",
       " -0.046906478703022,\n",
       " -0.21936295926570892,\n",
       " 0.0146942725405097,\n",
       " -0.006587575655430555,\n",
       " 0.06335298717021942,\n",
       " -0.035481829196214676,\n",
       " 0.0070517538115382195,\n",
       " 0.05168687179684639,\n",
       " 0.01193583570420742,\n",
       " -0.05624203383922577,\n",
       " 0.006103140767663717,\n",
       " -0.00836534146219492,\n",
       " 0.01947271265089512,\n",
       " -0.04787091538310051,\n",
       " 0.005564184859395027,\n",
       " 0.06194417178630829,\n",
       " -0.015278181992471218,\n",
       " 0.07348145544528961,\n",
       " -0.04409276321530342,\n",
       " 0.015433531254529953,\n",
       " -0.03178562596440315,\n",
       " 0.04244723916053772,\n",
       " -0.01340482197701931,\n",
       " 0.12835149466991425,\n",
       " -0.038138534873723984,\n",
       " 0.029754215851426125,\n",
       " -0.013811707496643066,\n",
       " -0.04606318101286888,\n",
       " 0.07093189656734467,\n",
       " 0.09106580168008804,\n",
       " -0.014139966107904911,\n",
       " -0.03332357853651047,\n",
       " -0.019939566031098366,\n",
       " 0.020638804882764816,\n",
       " -0.04228987172245979,\n",
       " 0.06073520705103874,\n",
       " -0.06163337081670761,\n",
       " -0.04282156378030777,\n",
       " 0.0016877060988917947,\n",
       " 0.015690702944993973,\n",
       " -0.022689491510391235,\n",
       " -0.0476088672876358,\n",
       " 0.025620905682444572,\n",
       " -0.024673184379935265,\n",
       " -0.011693754233419895,\n",
       " 0.040657058358192444,\n",
       " 0.005869096145033836,\n",
       " -0.027067163959145546,\n",
       " -0.036087360233068466,\n",
       " -0.038707125931978226,\n",
       " 0.03617168590426445,\n",
       " 0.010129719041287899,\n",
       " 0.00042515055974945426,\n",
       " -0.005032294429838657,\n",
       " 0.043418239802122116,\n",
       " 0.06080981716513634,\n",
       " 0.004822329618036747,\n",
       " -0.032541483640670776,\n",
       " -8.978043479146436e-05,\n",
       " -0.006149936001747847,\n",
       " -0.006878344342112541,\n",
       " -0.04559164494276047,\n",
       " -0.04992372542619705,\n",
       " 0.0693269819021225,\n",
       " -0.010948605835437775,\n",
       " 0.03456278517842293]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_texts([query])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "91a11298-ebd4-4302-811b-a209e468148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_upsert = list([\n",
    "    {\"id\": f\"{i}\", \"values\": vectors[i], \"metadata\": {\"text\": texts[i]}} \n",
    "    for i in range(len(texts))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "6bdb498f-99d4-4429-816f-74d6313905e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 993}"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.upsert(to_upsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e85eab3b-8445-4285-8ba5-4cefe24f58eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 254 ms, sys: 5.07 ms, total: 259 ms\n",
      "Wall time: 291 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "matches = index.query(\n",
    "    vector=embed_texts([query])[0],\n",
    "    top_k=3,\n",
    "    include_values=True,\n",
    "    include_metadata=True\n",
    ")['matches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6d11935-d8de-43c0-8ab0-b80d30941af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "\n",
      "Match score: 0.706573248\n",
      "Charge phone\\n\\nHow to charge phone\\n\\nYour phone is like a small computer, giving you tons of information and apps. Depending on what you use, \\nthat can take a lot of power. Plan on charging the phone each night so it's ready for the next day.\\n. Connect your phone to the charger that came with your phone.\\n\\nOther chargers, including laptops, can charge more slowly.\\n.\n",
      "-----------------------------------\n",
      "\n",
      "Match score: 0.671359897\n",
      "If you're charging by connecting to a wall outlet:\\n\\n•\\n\\n•\\n\\n•\\n\\n•\\n\\n•\\n\\nIf the outlet is controlled with a switch, check that it is turned on.\\n\\nUse the charger that came with the phone.\\n\\nRemove any case, to see if it's impeding the charger/port connection.\\n\\nInspect the charger for damage to the wire or plug. If you see damage, try charging by connecting \\nto your computer with a USB cable.\n",
      "-----------------------------------\n",
      "\n",
      "Match score: 0.670007408\n",
      "You can turn off charging sounds.\\n\\nCharge other devices\\nYour battery has enough power that you can use it to charge other compatible wireless devices, like \\nheadphones or other phones. Make sure the device you want to charge supports wireless charging.\\n. Open quick settings, then touch & hold \\n\\n.\\n\\nOr, go to Settings > Battery > Power sharing.\\n.\\n\\nTurn Allow power sharing on\n"
     ]
    }
   ],
   "source": [
    "for match in matches:\n",
    "    print('-'*35)\n",
    "    print()\n",
    "    print(f\"Match score: {match['score']}\")\n",
    "    print(match['metadata']['text'].replace(\"\\n\", \"\\\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e55152c-4f4c-4346-88c5-9899b844692f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libs  models  TinyLlama-1.1B-Chat-v1.0-MLC\n",
      "TinyLlama-1.1B-Chat-v1.0-q4f32_1-vulkan.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls ../dist\n",
    "!ls ../dist/libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0ad8d7a-b17e-4135-a303-62409e622b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlc_llm.support.auto_device:\u001b[91mNot found\u001b[0m device: cuda:0\n",
      "INFO:mlc_llm.support.auto_device:\u001b[91mNot found\u001b[0m device: rocm:0\n",
      "INFO:mlc_llm.support.auto_device:\u001b[91mNot found\u001b[0m device: metal:0\n",
      "INFO:mlc_llm.support.auto_device:\u001b[92mFound\u001b[0m device: vulkan:0\n",
      "INFO:mlc_llm.support.auto_device:\u001b[92mFound\u001b[0m device: vulkan:1\n",
      "INFO:mlc_llm.support.auto_device:\u001b[92mFound\u001b[0m device: vulkan:2\n",
      "INFO:mlc_llm.support.auto_device:\u001b[91mNot found\u001b[0m device: opencl:0\n",
      "INFO:mlc_llm.support.auto_device:Using device: \u001b[1mvulkan:0\u001b[0m\n",
      "INFO:mlc_llm.chat_module:Using model folder: /home/jgpt/luxai/llms/mlc-llm/dist/TinyLlama-1.1B-Chat-v1.0-MLC\n",
      "INFO:mlc_llm.chat_module:Using mlc chat config: /home/jgpt/luxai/llms/mlc-llm/dist/TinyLlama-1.1B-Chat-v1.0-MLC/mlc-chat-config.json\n",
      "INFO:mlc_llm.chat_module:Using library model: ../dist/libs/TinyLlama-1.1B-Chat-v1.0-q4f32_1-vulkan.so\n",
      "[2024-05-09 00:48:28] INFO model_metadata.py:96: \u001b[92mTotal memory usage\u001b[0m: 716.24 MB (Parameters: 590.24 MB. KVCache: 0.00 MB. Temporary buffer: 126.00 MB)\n",
      "[2024-05-09 00:48:28] INFO model_metadata.py:105: To reduce memory usage, tweak `prefill_chunk_size`, `context_window_size` and `sliding_window_size`\n"
     ]
    }
   ],
   "source": [
    "cm = ChatModule(model=\"../dist/TinyLlama-1.1B-Chat-v1.0-MLC\", model_lib_path=\"../dist/libs/TinyLlama-1.1B-Chat-v1.0-q4f32_1-vulkan.so\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33f8dc0d-bb7c-4193-81f5-31543042a5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To charge your phone, follow these steps:\n",
      "\n",
      "1. Unplug your phone from the wall adapter. 2. Insert the wall adapter into the wall outlet. 3. Plug the phone into the wall adapter. 4. Connect the phone to a power source. 5. The phone should now be charging. Note: Always unplug the wall adapter before charging your phone. Also, make sure your phone has enough battery to charge fully before attempting to connect it to a power source. If you need more information, please let me know and I'll be happy to provide you with more details!\n"
     ]
    }
   ],
   "source": [
    "print(cm.generate(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1f4ab7ec-97ab-465e-910f-ec5d07b3c3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.reset_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1baa17ad-feb3-441c-b499-89b86df6108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_formatter(query, context_list):\n",
    "    sep = \" [CONTEXT_SEP] \"\n",
    "    return f\"\"\"\"Please, use the context below to anwer the query. Use only the context discard all previous knowledge except user messages.\n",
    "Absorb the context as if it was not given. Answer honestly and in simple words. \n",
    "Do not necessarily use all of the context as it may contain irrelevant information.\n",
    "Reply with a step by step process to fulfill the user's needs.\n",
    "If you don't know the answer, simply state so. Here's the context:\n",
    "{sep}{sep.join(context_list)}{sep}\n",
    "Here's the query:\n",
    "{query}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d2b43741-5987-49a5-9982-1a04f2a3527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_list = list([match['metadata']['text'].replace(\"\\n\", \"\") for match in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f4cde096-5652-4b04-86da-0fa38360c66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Please, use the context below to anwer the query. Use only the context discard all previous knowledge except user messages.\n",
      "Absorb the context as if it was not given. Answer honestly and in simple words. \n",
      "Do not necessarily use all of the context as it may contain irrelevant information.\n",
      "Reply with a step by step process to fulfill the user's needs.\n",
      "If you don't know the answer, simply state so. Here's the context:\n",
      "\n",
      "\n",
      "Charge phoneHow to charge phoneYour phone is like a small computer, giving you tons of information and apps. Depending on what you use, that can take a lot of power. Plan on charging the phone each night so it's ready for the next day.. Connect your phone to the charger that came with your phone.Other chargers, including laptops, can charge more slowly..\n",
      "\n",
      "If you're charging by connecting to a wall outlet:•••••If the outlet is controlled with a switch, check that it is turned on.Use the charger that came with the phone.Remove any case, to see if it's impeding the charger/port connection.Inspect the charger for damage to the wire or plug. If you see damage, try charging by connecting to your computer with a USB cable.\n",
      "\n",
      "You can turn off charging sounds.Charge other devicesYour battery has enough power that you can use it to charge other compatible wireless devices, like headphones or other phones. Make sure the device you want to charge supports wireless charging.. Open quick settings, then touch & hold .Or, go to Settings > Battery > Power sharing..Turn Allow power sharing on\n",
      "\n",
      "\n",
      "Here's the query:\n",
      "How to charge?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_formatter(query, context_list).replace(\" [CONTEXT_SEP] \", \"\\n\\n\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a38af10b-ba76-4862-a94b-96b6f9db8bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a step-by-step process to charge a phone:\n",
      "\n",
      "1. Plug in the charger that came with the phone.\n",
      "2. Ensure the charger is connected to a wall outlet.\n",
      "3. Check if the outlet is turned on.\n",
      "4. If the outlet is controlled with a switch, check that it is turned on.\n",
      "5. Remove any case, to see if it's impeding the charger/port connection.\n",
      "6. Inspect the charger for damage to the wire or plug.\n",
      "7. If you see damage, try charging by connecting to your computer with a USB cable.\n",
      "8. Turn off charging sounds.\n",
      "9. Charge other devices by opening Quick Settings and touching & holding, then go to Settings > Battery > Power sharing.\n",
      "10. Turn Allow power sharing on.\n",
      "\n",
      "Hope this helps! Let me know if you have any more questions.\n"
     ]
    }
   ],
   "source": [
    "print(cm.generate(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64851dd8-4ec3-4ee3-9a95-686dc8127cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
